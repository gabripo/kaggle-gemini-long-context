{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9998649,"sourceType":"datasetVersion","datasetId":6154170}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gabripo93/the-perfect-match-for-your-tech-and-business-needs?scriptVersionId=209768245\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# In-chat Multiagents to Find the Right Company and Generate Clause-by-Clause Reports for Tenders 📑💼\n\nThe following Kaggle notebook takes advantage of Gemini's long context window to achieve the following objectives:\n\n- Analyze technical and commercial tenders for a project. 📊\n- Assess the compatibility of companies' products and solutions with tender documents. 🔍\n- Identify the best company and product-service combination to execute the project, generating a clause-by-clause report with compliant and non-compliant specifications. 📋✅❌\n\n## Notebook Structure 📓\n\nThe notebook is divided into different sections, each with a specific objective:\n\n- Dataset load (see *Relevant Project and Open points* chapter for data generation)\n- Tenders for a project are parsed, converting their information into text. 📝\n- Information scraped from various companies' websites is loaded as text. 🕸️\n- All text is processed by Gemini using different prompts, combining multi-agent reasoning, chain of thoughts, and in-chat memory. 🤖💭\n\n## Multi-agent Reasoning 🧠\n\nMulti-agent reasoning is implemented by segmenting tasks and delegating responsibilities to distinct roles. For example:\n\n- **Technical and Commercial Tender Agents**: Separate prompts (tender_prompt_template_technical and tender_prompt_template_commercial) guide the roles of the technical tender engineer and commercial tender manager. Each agent has distinct objectives: identifying and summarizing technical or commercial requirements within tenders. This multi-agent structure ensures detailed and domain-specific analyses. 👷‍♂️💼\n\n- A distinct prompt is also prepared for analyzing companies (e.g., SIEMENS and HITACHI) to match tender requirements with their products and solutions (get_response_companies_info). This allows tailored reasoning for comparing affinities between tenders and company offerings. 🏢🔄\n\n## Chain of Thoughts 🧩\n\nThe chain of thoughts approach decomposes complex tasks into sequential, step-by-step actions, ensuring methodical problem-solving. In both technical and commercial prompts, we used phrases like \"Think step by step\" to guide the agent toward incremental reasoning. This ensures that requirements are dissected and analyzed in detail. 🔍🧠\n\nThe user prompt specifies a structured approach to calculating an affinity score, prompting the agent to explicitly explain the calculation process. Finally, in the Clause-by-Clause Analysis, the final prompt directs the agent to meticulously compare tender requirements with company specifications, maintaining a clear progression in thought. This approach is embedded in the tender query processing and the affinity scoring logic in user_prompt_match and final_prompt, encouraging logical progression in the analysis. 📈🔗\n\n## In-chat Memory 🗃️\n\nThe code uses in-chat memory to maintain conversational context across multiple interactions.\n\nIn-chat memory stores all the interactions from the technical and commercial tender analysis, keeping track of the responses from the different roles (e.g., technical engineer, commercial manager, sales manager). This memory allows to build upon the context of earlier prompts without having to constantly reprocess the same information. With context caching the system stores intermediate results from prior tender evaluations or company analyses, so if a similar query arises, the system can quickly retrieve relevant data and produce faster, more accurate responses.\n\nThis functionality is facilitated by:\n\n- **Chat History Preservation**: The function add_history_to_chat appends user queries and model responses (e.g., for tenders or company analyses) to history_chat. This ensures continuity, enabling the model to refer back to previous inputs and outputs during subsequent exchanges. 📝🔄\n\n- Prompts such as system_prompt and user_prompt leverage the accumulated chat history to enhance the depth and relevance of responses. For example, when computing affinity scores or performing a clause-by-clause analysis, the model can reference earlier content in the chat_with_memory object. This allows continuous improvement of the prompt and on the information stored in the chat. 🗣️🔍\n\n## Conclusion for the Use Case 🤔\n\nUsing a long context window instead of Retrieval-Augmented Generation (RAG) for this use case was particularly beneficial due to the task's nature, which involves reasoning across interdependent documents while maintaining conversational continuity and ensuring consistent context for decision-making. The unified context allows the model to cross-reference tender requirements and company offerings directly, ensuring cohesive and accurate analysis. This is particularly advantageous for tasks like affinity scoring, which require simultaneous consideration of multiple data points. 📊🔗\n\nThe notebook's approach scales better for handling multiple queries simultaneously, as it avoids the bottleneck of sequential agent calls. For new tender projects, it's only necessary to update the in-chat memory and add new prompts for adding new in-chat agents. 🔄🔄\n\nIn summary, why did we decide to build this notebook?\n\n1. **Holistic Context Retention** 📚: By storing the entire history of tender analyses (both technical and commercial) and company product evaluations, the model retains a comprehensive understanding of all previously provided information. This holistic context allows the model to reason about how specific requirements and offerings interrelate across multiple prompts. In RAG, the system retrieves only the most relevant chunks of information for each query; this efficient approach can lead to fragmented analyses, potentially overlooking interconnections.\n\n2. **Interdependent Analysis** 🔄: This task involves comparing multiple tenders against products and solutions offered by different companies, followed by calculating an affinity score and conducting a clause-by-clause compliance analysis. These steps require accessible and integrated information from previous steps. RAG typically retrieves context independently for each query, which might result in a loss of nuance or context-dependent reasoning, especially when relationships between multiple documents must be preserved. A long context window ensures the model has immediate access to the entire conversational flow and insights developed so far.\n\n3. **Dynamic Multi-Agent Collaboration** 🤝: By maintaining a long context, the system can simulate multi-agent collaboration, allowing outputs from technical engineers, commercial managers, and sales managers to flow into a unified reasoning framework. In RAG, each role’s analysis would require re-retrieving relevant information, possibly leading to inconsistencies or duplications. A long context window naturally informs each role, creating a seamless chain of thought.\n\n4. **Reduced Query Overhead** 🔄: Long context windows reduce the need for multiple retrieval calls, making the process more efficient in scenarios where information is revisited or refined iteratively. RAG introduces latency and computational costs because each query requires searching and ranking document chunks. A long context window allows for continuous focus on the task, with all prior exchanges readily available.\n\n5. **Affinity Score Calculation** 📈: Computing an affinity score across companies for tenders requires integrating technical and commercial analysis alongside company data. This step benefits significantly from the model's ability to access all previous responses simultaneously. In RAG, affinity scoring would require separate retrievals of technical requirements, commercial requirements, and company data for each tender. This could introduce discrepancies if context for one query is inadvertently excluded during retrieval.\n\n6. **Clause-by-Clause Compliance Analysis** 🔄: Clause-by-clause analysis relies on cross-referencing previously extracted requirements with company offerings. The long context window allows the model to directly reference earlier inputs and outputs without reloading or retrieving. RAG retrievals for clause-by-clause analysis might lead to inconsistencies if prior reasoning is split across multiple retrievals. A long context window ensures the model \"remembers\" and applies earlier analyses cohesively.\n\n### Related Projects and Open Points 📁\n\nThe data generation and cleaning is performed with another repo stored in github:  https://github.com/gabripo/kaggle-gemini-long-context.\n\nIn the past few months, we also implemented a multi-agent framework (LumadaAI) using LangChain and OpenAI, where each company was represented by a dedicated agent. **LumadaAI** is publicly available at https://github.com/SecchiAlessandro/LumadaAI. This framework featured a supervisor agent that dynamically routed user queries to the most relevant company-specific agent based on the query context. While innovative, this approach faced challenges in stability, accuracy, and efficiency, making the current solution more effective. As agents operated independently, generating combined solutions from different companies was difficult. Additionally, for each query, the supervisor needed to perform additional reasoning before invoking an agent. If a query was relevant to multiple agents, the framework had to perform sequential calls, compounding latency. The current solution with centralized reasoning ensures consistent application of logic and context. By avoiding the intermediate step of agent selection, it directly processes queries with unified context, reducing latency significantly. \n\n**EasyRAG** (https://github.com/gabripo/easyrag) is another RAG tool that performs RAG over locally stored documents. We are benchmarking this tool with Gemini's long context window: adding one or more PDFs to Gemini's context window could provide more precise insights than the RAG approach. 📈\n\n## Conclusion 🔽\n\nThe centralized, long context window approach provides clear advantages in stability, response time, and accuracy over the earlier multi-agent framework. It highlights the importance of selecting a system architecture that aligns with the specific demands of the use case, particularly for complex, multi-faceted analyses like those in tender evaluations, clause-by-clause generation, and company affinity scoring. 📊🔗\n\n#### The long context window acts as a shared workspace, recording and making all agent outputs accessible for seamless and holistic reasoning. In today's interconnected world, where partnerships and synergies are essential to addressing complex challenges, we envision a tool that enables continuous reasoning, uncovers new patterns and solutions, and minimizes the fragmentation of insights. 🌐🔍💡\n\n","metadata":{}},{"cell_type":"code","source":"# import Python libraries\nimport os\nimport json\nfrom IPython.display import Markdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# auxiliary function to read JSON files\ndef read_json_info(jsonFilePath: str) -> dict:\n    if os.path.exists(jsonFilePath):\n        with open(jsonFilePath, \"r\") as f:\n            data = json.load(f)\n        return data\n    else:\n        return {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# auxiliary Python decorator to execute a function again, if its execution fails\n# this is helpful when calling the Gemini's API since Gemini has a rate limiter and, if an execution fails for that, there will be some waiting time before retrying\nimport time\n\ndef retry_on_failure(wait_time_seconds=60, max_retries=5):\n    def decorator_retry(func):\n        \n        def wrapper_retry(*args, **kwargs):\n            retries = 0\n            while retries < max_retries:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    retries += 1\n                    if retries < max_retries:\n                        print(\n                            f\"Function failed with error: {e}. Retrying in {wait_time_seconds} seconds... (Attempt {retries}/{max_retries})\"\n                        )\n                        time.sleep(wait_time_seconds)\n                    else:\n                        print(f\"Function failed after {max_retries} attempts.\")\n                        raise e\n        return wrapper_retry\n\n    return decorator_retry","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = '/kaggle/input/tenders-and-companies-websites'\nworking_path = '/kaggle/working'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/tenders\ntenders_working_path = os.path.join(working_path, 'tenders')\n\n!mkdir -p /kaggle/working/companies\ncompanies_working_path = os.path.join(working_path, 'companies')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build a chat with Gemini","metadata":{}},{"cell_type":"code","source":"# API key got here: https://ai.google.dev/tutorials/setup\n\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n\nuser_secrets = UserSecretsClient()\nsecret_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\ngenai.configure(api_key = secret_key)\n\nmodel_name = 'gemini-1.5-flash'\nmodel = genai.GenerativeModel(model_name=model_name)\n\nmodel_info = genai.get_model(f\"models/{model_name}\")\nprint(f\"{model_info.input_token_limit=}\")\nprint(f\"{model_info.output_token_limit=}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"List of models that support generateContent:\\n\")\nfor m in genai.list_models():\n    if \"generateContent\" in m.supported_generation_methods:\n        print(m.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the decorator ensures that, if an error occurs, the function will be executed again\n@retry_on_failure(wait_time_seconds=60, max_retries=5)\ndef ask_gemini(prompt, chat_with_memory=None, model=None, history=[], model_name = 'gemini-1.5-flash-latest'):\n    \"\"\"\n    function to call Gemini, providing chat history\n    if a chat is already available, it will be used\n    \"\"\"\n    if model == None:\n        model = genai.GenerativeModel(model_name=model_name)\n        \n    if chat_with_memory == None:\n        # since no chat is already available, create a new one\n        chat_with_memory = model.start_chat(history=history)\n    \n    response = chat_with_memory.send_message(prompt)\n    return response, chat_with_memory","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# initialize the response dictionary\nresponses = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analyze the tenders","metadata":{}},{"cell_type":"code","source":"# read the json file related to tenders from the input dataset\ntenders_info_json_path = os.path.join(dataset_path, 'tenders_info.json')\ntenders_info = read_json_info(tenders_info_json_path)\n\n# tenders_info is a dictionary, where the key is the name of the tender file and the related value its information\n# print(tenders_info[\"tender_wind.pdf\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list the processed tender files\ntenders = tenders_info.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tender_prompt_template_technical = \"\"\"\nYou are an experienced technical tender engineer. \nThe document you have is a tender, that contains also technical requirements for a project.\nThink step by step on how to look for the relevant technical requirements and make a detailed summary.\nThe content of the document is: \"\"\"\ntender_prompts_technical = []\nfor info in tenders_info.values():\n    tender_prompts_technical.append(f\"You have a document called {info['name']} . \" + tender_prompt_template_technical + f\"{info['content']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tender_prompt_template_commercial = \"\"\"\nYou are an experienced commercial tender manager. \nThe document you have is a tender, that contains also commercial requirements for a project.\nThink step by step on how to look for the relevant commercial requirements and make a detailed summary.\nThe content of the document is: \"\n\"\"\"\ntender_prompts_commercial = []\nfor info in tenders_info.values():\n    tender_prompts_commercial.append(f\"You have a document called {info['name']} . \" + tender_prompt_template_commercial + f\"{info['content']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@retry_on_failure(wait_time_seconds=60)\ndef get_responses_tenders(subject, tender_prompts):\n    tenders_json_file_path = os.path.join(tenders_working_path, f'tenders_{subject}.json')\n    \n    if os.path.exists(tenders_json_file_path):\n        responses = read_json_info(tenders_json_file_path)\n        print(f\"tender_{subject}: Responses loaded from file {tenders_json_file_path}\")\n    else:\n        for tender_prompt, tender_name in zip(tender_prompts, tenders):\n            print(f\"tender_{subject}: Generating response for tender {tender_name} ...\")\n            response, _ = ask_gemini(prompt = tender_prompt)\n            #print(response.text)\n\n            responses = {}\n            responses[tender_name] = {'prompt': tender_prompt, 'answer': response.text}\n            print(f\"tender_{subject}: Response for tender {tender_name} generated.\")\n    \n        with open(tenders_json_file_path, 'w') as f:\n            json.dump(responses, f, ensure_ascii=True, indent=4)\n        print(f\"tender_{subject}: Responses stored into {tenders_json_file_path}\")\n    \n    print(f\"tender_{subject}: Analysis concluded!\\n\")\n    return responses\n\n# each call of get_responses_tenders() will generate a tenders_{subject}.json file\n# each generated file so will contain the Gemnini's responses for a given subject\nresponses['tender_technical'] = get_responses_tenders(\"technical\", tender_prompts_technical)\nresponses['tender_commercial'] = get_responses_tenders(\"commercial\", tender_prompts_commercial)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analyze the companies products and solutions","metadata":{}},{"cell_type":"markdown","source":"## Data generation\nInformation about interesting companies is obtained from their websites.\n\nTo generate data out of the companies' websites, we implemented a crawler.\n\nThe final output of the crawler is a JSON file, in which each field refers to a company: for each company, all the information of the websites is merged.\n\n> The generation of information can be found in the Kaggle Notebook https://www.kaggle.com/code/gabripo93/gemini15-long-context-competition-generate-dataset\n\n### Details about the crawling process:\n- **Recursive scan**: after a webpage is scanned and its content is stored, eventual found sublinks are scanned, as well. A limit of the wepages to download is given as input.\n- **Redundant information is deleted**: if some website content can be found multiple times in all the webpages of one company, then it is skipped. *Example*: undesired and redundant lines like \"Contact Us\" are removed, ensuring that the final content does not include unnecessary sentences.\n- **Caching of already downloaded pages**: for each webpage, the content is stored in a JSON file, as well as the found sublinks. *Example*: after a run with a limit of N pages, other runs with less than N pages will use the stored files instead downloading data from internet; at the contrary, if the limit is increased to M > N pages, only M - N additional pages will be downloaded while the first N pages will be taken from the stored file. ","metadata":{}},{"cell_type":"code","source":"companies_info_json_path = os.path.join(dataset_path, 'companies_info.json')\ncompanies_info = read_json_info(companies_info_json_path)\n\n# companies_info is a dictionary, where the key is the name of the company and the related value its information\n# print(companies_info[\"SIEMENS\"]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@retry_on_failure(wait_time_seconds=60)\ndef get_response_companies(company_name):\n    companies_json_file_path = os.path.join(companies_working_path, f'companies_{company_name}.json')\n    \n    if os.path.exists(companies_json_file_path):\n        responses = read_json_info(companies_json_file_path)\n        print(f\"companies_{company_name}: Responses loaded from file {companies_json_file_path}\")\n    else:\n        print(f\"companies_{company_name}: Generating response for company {company_name} ...\")\n        responses = {}\n        company_prompt = f\"These are the information of products and solutions for the company {company_name} : {companies_info[company_name]}\"\n        response, _ = ask_gemini(prompt = company_prompt)\n        responses[company_name] = {'prompt': company_prompt, 'answer': response.text}\n\n        with open(companies_json_file_path, 'w') as f:\n                json.dump(responses, f, ensure_ascii=True, indent=4)\n        print(f\"companies_{company_name}: Responses stored into {companies_json_file_path}\")\n\n    print(f\"companies_{company_name}: Response for company {company_name} generated!\")\n    return responses\n\n# each call of get_responses_companies() will generate a companies_{company_name}.json file\n# each generated file so will contain the Gemnini's responses for a given company\n# the purpose of generating responses given companies information is to store it in the chat history\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"responses['company_hitachi'] = get_response_companies(\"HITACHI\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"responses['company_siemens'] = get_response_companies(\"SIEMENS\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build a chat history based on previous prompts","metadata":{}},{"cell_type":"code","source":"# example how to include the chat history here https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_chat.ipynb\n# description of the Content class here https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/GenerativeModel.md\nfrom google.generativeai.protos import Content, Part\n\nhistory_chat = []\n\ndef add_history_to_chat_single(response, user, history_chat):\n    query = Part()\n    query.text = f\"{user}: {response['prompt']}\"\n    history_chat.append(Content(role=\"user\", parts=[query]))\n\n    answer = Part()\n    answer.text = response['answer']\n    history_chat.append(Content(role=\"model\", parts=[answer]))\n    return\n\ndef add_history_to_chat(responses, user, history_chat):\n    for response in responses.values():\n        add_history_to_chat_single(response, user, history_chat)\n    return \n\nadd_history_to_chat(responses['tender_technical'], \"technical engineer\", history_chat)\nadd_history_to_chat(responses['tender_commercial'], \"commercial manager\", history_chat)\nadd_history_to_chat(responses['company_siemens'], \"sales manager for siemens\", history_chat)\nadd_history_to_chat(responses['company_hitachi'], \"sales manager for hitachi\", history_chat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the chat history","metadata":{}},{"cell_type":"code","source":"prompt_roles = \"Which are the roles given in the prompt from the user? There are only two for tenders and one for company\"\nresponses['prompt_roles'], gemini_chat = ask_gemini(prompt=prompt_roles, history=history_chat)\n\nMarkdown(responses['prompt_roles'].text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add the last response to the chat history\nadd_history_to_chat_single({'prompt': prompt_roles, 'answer': responses['prompt_roles'].text}, \"technical engineer\", history_chat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Find the most suitable company","metadata":{}},{"cell_type":"code","source":"prompt_match = \"\"\"\n\n1. For companies SIEMENS and HITACHI, find the relevant products and solutions with respect to the analyzed tenders. The information is in the form of text I provided, then you do not need to read additional documents or access to websites. Report the corresponding URL at least once when you mention a product or a solution.\n   \n2. Calculate an affinity score in percentage for each company based on analysis in point 1 . Explain the way how you computed the affinity score. When possible, use tables and other effective representation ways to summrize numbers and specific information. If there is not product or solution to be used to fulfill a requirement, mention it and use the provided text to propose an alternative: for each of its elements, provide the URL at which the product or solution can be found; the URL for each product or solution is provided as text; if no solutions are viable with the information you have, mention it.\n\n\"\"\"\n\nMarkdown(prompt_match)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Finding the most suitable company for the tenders ...\")\nresponses['prompt_match'], gemini_chat = ask_gemini(prompt=prompt_match, chat_with_memory=gemini_chat)\nprint(\"Response to the prompts is ready!\")\n\nMarkdown(responses['prompt_match'].text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add the last response to the chat history\nadd_history_to_chat_single({'prompt': prompt_match, 'answer': responses['prompt_match'].text}, \"technical engineer\", history_chat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate the clause-by-clause","metadata":{}},{"cell_type":"code","source":"user_prompt = \"\"\"\n\nConsider the company with the highest affinity score \nand return the clause by clause analysis considering technical and commercial compliant and not-compliant requirements\nof the tender with respect to the selected company. For each company, report the URL of the source where you found information about mentioned products and solutions.\nWhen possible, show data and explain the reasons behind your thinking in tables.\n\n\"\"\"\n\nMarkdown(user_prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system_prompt = \"\"\"\nYou are an experienced team of business development managers and tender engineers, commercial managers.\nYou need to create a detailed clause by clause from the tender documentations and the most affine company specifications.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Generating the clause by clause ...\")\nprompt_clause_by_clause = f\"{system_prompt} {user_prompt}\"\nresponses['prompt_clause_by_close'], gemini_chat = ask_gemini(prompt=prompt_clause_by_clause, chat_with_memory=gemini_chat)\nprint(\"Response to the prompts is ready!\")\n\nMarkdown(responses['prompt_clause_by_close'].text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add the last response to the chat history\nadd_history_to_chat_single({'prompt': prompt_clause_by_clause, 'answer': responses['prompt_clause_by_close'].text}, \"technical engineer\", history_chat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Count the overall tokens","metadata":{}},{"cell_type":"markdown","source":"The total number of token can be computed by counting the tokens of history_chat, since the new responses have been appended to it for each call of Gemini.","metadata":{}},{"cell_type":"code","source":"print(f\"{model.count_tokens(history_chat)=}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the responses into an output file","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/output\n\noutput_file_name = 'kaggle_output.json'\nwith open(os.path.join('/kaggle/working/output', output_file_name)) as file:\n    json.dump(responses, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Alternative usage of Gemini: Context caching","metadata":{}},{"cell_type":"markdown","source":"As the entire dataset consists in JSON files, it could be cached using the Context caching functionality of Gemini:\n\nhttps://ai.google.dev/gemini-api/docs/caching?lang=python\n\nWith context caching the system stores intermediate results from prior tender evaluations or company analyses, so if a similar query arises, the system can quickly retrieve relevant data and produce faster, more accurate responses.","metadata":{}},{"cell_type":"code","source":"# to ensure that no caching limit is exceeded, flush all the already available caches\nfrom google.generativeai import caching\n\ndef delete_caches() -> None:\n    for c in caching.CachedContent.list():\n        print(f\"Deleting cache named \\\"{c.display_name}\\\" ...\")\n        c.delete()\n    print(\"All the caches have been deleted!\")\n\ndelete_caches()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nimport time\n\n@retry_on_failure(wait_time_seconds=10, max_retries=2)\ndef process_file_for_caching(file_path: str) -> genai.types.file_types.File | None:\n    if os.path.exists(file_path):\n        loaded_file = genai.upload_file(file_path)\n        \n        while loaded_file.state.name == \"PROCESSING\":\n            print(f\"Processing file {file_path} ...\")\n            time.sleep(2)\n            loaded_file = genai.get_file(loaded_file.name)\n        print(f\"Processing file {file_path} completed. Available at {loaded_file.uri}\")\n        \n        return loaded_file\n    else:\n        return None\n\n# the process_file_for_caching() function will make the input file as available for caching\ntenders_info_file = process_file_for_caching(tenders_info_json_path)\ncompanies_info_file = process_file_for_caching(companies_info_json_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cache_instructions = f\"\"\"\n{system_prompt}\nThe information you need is in the JSON files you have access to.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datetime\n\n@retry_on_failure(wait_time_seconds=60, max_retries=2)\ndef build_cache_from_contents(model_name: str ='gemini-1.5-flash-002', cache_name: str='cache', instructions: str=\"Use the information to answer\", resources: list=[], minutes_available: int=10):\n    cache = caching.CachedContent.create(\n        model=model_name,\n        display_name=cache_name, # used to identify the cache\n        system_instruction=(instructions),\n        contents=resources,\n        ttl=datetime.timedelta(minutes=minutes_available),\n    )\n    \n    model_with_cache = genai.GenerativeModel.from_cached_content(cached_content=cache)\n    return model_with_cache\n\nfiles_to_cache = [tenders_info_file, companies_info_file]\nmodel_with_cache = build_cache_from_contents(cache_name='tenders and companies info', instructions=cache_instructions, resources=files_to_cache)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_available_caches():\n    cache_list = list(caching.CachedContent.list())\n    if len(cache_list) == 0:\n        print(\"Empty cache!\")\n        return []\n        \n    for c in cache_list:\n        print(f\"Available cache with name \\\"{c.display_name}\\\"\\n    model: {c.model}\\n    created: {c.create_time}\\n    expires: {c.expire_time}\\n    tokens: {c.usage_metadata.total_token_count}\")\n        # print(c)\n    return cache_list\n\ncaches = list_available_caches()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test if the cache works","metadata":{}},{"cell_type":"code","source":"def ask_gemini_with_cache_dummy_response(model, prompt: str=\"\"):\n    \"\"\"\n    Function to handle a possible error by Gemin when generating a response\n    \"\"\"\n    try:\n        response = model.generate_content(prompt)\n    except Exception as exc:\n        print(f\"Execution in model with caching failed: {exc}\")\n        print(\"An empty response will be returned\")\n        \n        class DummyResponse:\n            def __init__(self):\n                self.text = \"MODEL WITH CACHING FAILED TO GENERATE RESPONSE\"\n        response = DummyResponse()\n        \n    return response\n\n@retry_on_failure(wait_time_seconds=2, max_retries=5)\ndef ask_gemini_with_cache(model, prompt: str=\"\"):\n    \"\"\"\n    Wrapper for a Gemini model that uses cache\n    \"\"\"\n    response = model.generate_content(prompt)\n    return response","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response_cached_files = ask_gemini_with_cache_dummy_response(model=model_with_cache, prompt=\"Describe the documents you have access to\")\n\nMarkdown(response_cached_files.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate the clause-by-clause with cached content","metadata":{}},{"cell_type":"code","source":"response_cached_clause_by_clause = ask_gemini_with_cache_dummy_response(model=model_with_cache, prompt=f\"Use the documents you have access to, to answer.\\n {user_prompt}\")\n\nMarkdown(response_cached_clause_by_clause.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deleting the generated caches","metadata":{}},{"cell_type":"code","source":"delete_caches()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}