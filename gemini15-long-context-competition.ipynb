{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tender2Project\nThe following Kaggle notebook exploits the long context window of Gemini, in order to fulfill the following targets:\n- Analyze technical and commercial tenders for a project.\n- Analyze the compatibility of products and solution of certain companies with respect to the tender documentations.\n- Find the best company and combination of products and services to build the project, generating a clause by clause report with compliant and not-compliant specifications.\n\n## Notebook structure\nThe notebook is composed by different parts, each one with a specific target:\n- Tenders for a project are parsed, such that their information is converted to text.\n- Information scraped from the websites of different companies is loaded as text.\n- All the text is forwarded to Gemini in different prompts: combining multi-agent reasoning, chain of toughts and in-chat memory.\n\n## Multi-agent reasoning\nMulti-agent reasoning is applied in the code through the segmentation of tasks and the delegation of specific responsibilities to distinct roles. For example:\n\nTechnical and Commercial Tender Agents: Separate prompts (tender_prompt_template_technical and tender_prompt_template_commercial) are used to guide the technical tender engineer and the commercial tender manager roles, respectively. Each agent has distinct objectives: identifying and summarizing technical or commercial requirements within tenders. This multi-agent structure ensures detailed and domain-specific analyses.\n\nA distinct prompt is also prepared for analyzing companies (e.g., SIEMENS and HITACHI) to match tender requirements with their products and solutions (get_response_companies_info). This allows tailored reasoning for comparing affinity between tenders and company offerings.\n\n## Chain of Thoughts\nThe chain of thoughts approach is used to decompose complex tasks into sequential, step-by-step actions, ensuring methodical problem-solving. \nIn both technical and commercial prompts we used phrases like \"Think step by step\" to guide the agent toward incremental reasoning. This ensures that requirements are dissected and analyzed in detail.\nThe user prompt specifies a structured approach to calculating an affinity score, prompting the agent to explicitly explain the calculation process.\nFinally in the Clause-by-Clause Analysis, the final prompt directs the agent to meticulously compare tender requirements with company specifications, maintaining a clear progression in thought.\nThis approach is embedded in the query processing of tenders and the affinity scoring logic in user_prompt_match and final_prompt, encouraging logical progression in the analysis.\n\n## In-chat memory\nThe code utilizes in-chat memory to maintain conversational context across multiple interactions. This functionality is facilitated by Chat History Preservation: The function add_history_to_chat appends user queries and model responses (e.g., for tenders or company analyses) to history_chat. This ensures continuity, enabling the model to refer back to previous inputs and outputs during subsequent exchanges.\nAdditionally, prompts such as system_prompt and user_prompt leverage the accumulated chat history to enhance the depth and relevance of responses. For example, when computing affinity scores or performing a clause-by-clause analysis, the model can reference earlier content in the chat_with_memory object. This allows a continous improvement of the prompt and on the information stored in the chat.\n\n\n\n## Conclusion for use case\nUsing a long context window instead of Retrieval-Augmented Generation (RAG) for the selected use case was particularly beneficial due to the nature of the task, which involves reasoning across interdependent documents, maintaining conversational continuity, and ensuring consistent context for decision-making. \nIn fact, in the past, we implemented a multi-agent framework using LangChain and OpenAI, where each company was represented by a dedicated agent. The repo is publicily availbla at https://github.com/SecchiAlessandro/LumadaAI. This framework was designed with a supervisor agent that dynamically routed user queries to the most relevant company-specific agent based on the query context. While innovative for that time, this approach encountered several challenges, particularly in stability, accuracy, and efficiency, making the current solution implemented in the notebook a more effective alternative. \nAs the agents are operating independently, it was also difficult to generate combined solutions from different companies.\nMoreover, for each query, the supervisor had to perform an additional step of reasoning before invoking an agent.\nIf the query was relevant to multiple agents, the framework had to perform multiple sequential calls, compounding the latency.\n\nThe current solution with a centralized reasoning, ensure consistent application of logic and context.\nBy avoiding the intermediate step of agent selection, it directly processes queries with a unified context, significantly reducing latency.\n\nThe unified context allows the model to cross-reference tender requirements and company offerings directly, ensuring a cohesive and accurate analysis.\nThis is particularly advantageous for tasks like affinity scoring, which require simultaneous consideration of multiple data points.\n\nThe notebook’s approach scales better for handling multiple queries simultaneously, as it avoids the bottleneck of sequential agent calls. In fact, for new tender projects, it is just required to update the in-chat memory and to add new prompts for adding new in-chat agents.\n\n\nIn summary, why we decided to implement tender2project?\n\n1. Holistic Context Retention\nLong Context Window: By storing the entire history of tender analyses (both technical and commercial) and company product evaluations, the model retains a comprehensive understanding of all previously provided information. This holistic context allows the model to reason about how specific requirements and offerings interrelate across multiple prompts.\nIn RAG, the system retrieves only the most relevant chunks of information from the documents for each query. While efficient, this approach can fragment the analysis when tasks require synthesizing insights across documents, potentially leading to overlooked interconnections.\n2. Interdependent Analysis\nTender and Company Matching: The task involves comparing multiple tenders against products and solutions offered by different companies, followed by calculating an affinity score and conducting a clause-by-clause compliance analysis. These steps require information from previous steps to be accessible and integrated seamlessly.\nRAG typically retrieves context independently for each query, which might result in loss of nuance or context-dependent reasoning, especially when relationships between multiple documents must be preserved. A long context window ensures the model has immediate access to the entire conversational flow and insights developed so far.\n3. Dynamic Multi-Agent Collaboration\nRole-Based Prompts: By maintaining a long context, the system can simulate multi-agent collaboration, where outputs from technical engineers, commercial managers, and sales managers flow into a unified reasoning framework.\nIn RAG, each role’s analysis would require re-retrieving relevant information from documents, potentially leading to inconsistencies or duplications. With a long context window, outputs from one role naturally inform others, creating a seamless chain of thought.\n4. Reduced Query Overhead\nFewer Retrievals, Continuous Focus: Long context windows reduce the need for multiple retrieval calls, making the process more efficient in scenarios where information is revisited or refined iteratively.\nRAG introduces latency and computational costs because each query requires searching and ranking document chunks. A long context window allows for continuous focus on the task, with all prior exchanges readily available.\n5. Affinity Score Calculation\nGlobal Context for Consistent Metrics: Computing an affinity score across companies for tenders requires integrating technical and commercial analysis alongside company data. This step benefits significantly from the model’s ability to access all previous responses simultaneously.\nIn RAG, affinity scoring would require separate retrievals of technical requirements, commercial requirements, and company data for each tender. This could introduce discrepancies if context for one query is inadvertently excluded during retrieval.\n6. Clause-by-Clause Compliance Analysis\nIntegrated Insight Application: Clause-by-clause analysis relies on cross-referencing previously extracted requirements with company offerings. The long context window allows the model to directly reference earlier inputs and outputs without reloading or retrieving.\nRAG retrievals for clause-by-clause analysis might lead to inconsistencies if prior reasoning is split across multiple retrievals. A long context window ensures the model \"remembers\" and applies earlier analyses cohesively.\nTrade-offs and Use Case Fit\n\n## Conclusion\nThe centralized, long context window approach provides clear advantages in stability, response time, and accuracy over the earlier multi-agent framework. It highlights the importance of selecting a system architecture that aligns with the specific demands of the use case, particularly for complex, multi-faceted analyses like those in tender evaluations, clause-by-clause generation and company affinity scoring.\n#### The long context window acts as a shared workspace, recording and making all agent outputs accessible for seamless and holistic reasoning. In today's interconnected world, where partnerships and synergies are essential to addressing complex challenges, we envision a tool that enables continuous reasoning, uncovers new patterns and solutions, and minimizes the fragmentation of insights.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Clean the working directory","metadata":{}},{"cell_type":"code","source":"! rm -r /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:00.323799Z","iopub.execute_input":"2024-11-23T22:58:00.324193Z","iopub.status.idle":"2024-11-23T22:58:01.542012Z","shell.execute_reply.started":"2024-11-23T22:58:00.324160Z","shell.execute_reply":"2024-11-23T22:58:01.540359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Markdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:01.544773Z","iopub.execute_input":"2024-11-23T22:58:01.545282Z","iopub.status.idle":"2024-11-23T22:58:01.552399Z","shell.execute_reply.started":"2024-11-23T22:58:01.545225Z","shell.execute_reply":"2024-11-23T22:58:01.550721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the tenders","metadata":{}},{"cell_type":"markdown","source":"Locate the tenders - in PDF format.","metadata":{}},{"cell_type":"code","source":"# fetch the script to download content from GitHub\n!wget https://raw.githubusercontent.com/gabripo/kaggle-gemini-long-context/refs/heads/main/github_downloader.py -P /kaggle/working/scripts\n\n# add downloaded script to the Python path\nimport sys\nsys.path.append('/kaggle/working/scripts')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:01.554597Z","iopub.execute_input":"2024-11-23T22:58:01.555094Z","iopub.status.idle":"2024-11-23T22:58:03.224587Z","shell.execute_reply.started":"2024-11-23T22:58:01.555044Z","shell.execute_reply":"2024-11-23T22:58:03.223358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import github_downloader\n\ngithub_downloader.download_files_from_github_repo(folderName=\"tenders\", saveFolder=\"/kaggle/working/tenders\", extension=\"pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:03.228242Z","iopub.execute_input":"2024-11-23T22:58:03.228781Z","iopub.status.idle":"2024-11-23T22:58:04.979501Z","shell.execute_reply.started":"2024-11-23T22:58:03.228723Z","shell.execute_reply":"2024-11-23T22:58:04.978181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# if the PDF file is given as Kaggle Input (for example, manually uploaded), change the use_kaggle_input to True\nuse_kaggle_input_tender = False if os.path.exists('/kaggle/working/tenders') else True\nif use_kaggle_input_tender:\n    tenders_file_path = '/kaggle/input/tenders'\nelse:\n    tenders_file_path = '/kaggle/working/tenders'\n\nprint(f\"The folder {tenders_file_path} will be considered as containing the tenders\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:04.981020Z","iopub.execute_input":"2024-11-23T22:58:04.981449Z","iopub.status.idle":"2024-11-23T22:58:04.988469Z","shell.execute_reply.started":"2024-11-23T22:58:04.981399Z","shell.execute_reply":"2024-11-23T22:58:04.987367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Installing required Python packages to analyze the tender - in PDF format.","metadata":{}},{"cell_type":"code","source":"!pip install PyPDF2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:04.989990Z","iopub.execute_input":"2024-11-23T22:58:04.990436Z","iopub.status.idle":"2024-11-23T22:58:15.137341Z","shell.execute_reply.started":"2024-11-23T22:58:04.990382Z","shell.execute_reply":"2024-11-23T22:58:15.136114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Extract information from the tenders.\nThe output will be a text.","metadata":{}},{"cell_type":"code","source":"import os\nfrom PyPDF2 import PdfReader\n\ntenders = [t for t in os.listdir(tenders_file_path) if t.endswith(\".pdf\")]\ntenders_info = {}\nfor tender in tenders:\n    print(f\"Reading the tender {tender} ...\")\n    reader = PdfReader(os.path.join(tenders_file_path, tender))\n\n    tenders_info[tender] = {}\n    tenders_info[tender][\"name\"] = tender\n    tenders_info[tender][\"content\"] = \"\\n\".join([page.extract_text() for page in reader.pages])\n#Markdown(tenders_info[\"tender_solar.pdf\"][\"content\"])\n# information for each tender can be accessed by:\n#tenders_info[\"tender_solar.pdf\"][\"content\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:15.139037Z","iopub.execute_input":"2024-11-23T22:58:15.139464Z","iopub.status.idle":"2024-11-23T22:58:16.935392Z","shell.execute_reply.started":"2024-11-23T22:58:15.139416Z","shell.execute_reply":"2024-11-23T22:58:16.934255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fetch information about companies","metadata":{}},{"cell_type":"markdown","source":"## Overview\nInformation about interesting companies is obtained from their websites.\n\nTo generate data out of the companies' websites, we implemented a crawler.\nThe final output of the crawler is a JSON file, in which each field refers to a company: for each company, all the information of the websites is merged.\n\n> To make things easier, the mentioned JSON file will be fetched from a Git repository where the crawling function has already been executed.\n\n## Details about the crawling process:\n- **Recursive scan**: after a webpage is scanned and its content is stored, eventual found sublinks are scanned, as well. A limit of the wepages to download is given as input.\n- **Redundant information is deleted**: if some website content can be found multiple times in all the webpages of one company, then it is skipped. *Example*: undesired and redundant lines like \"Contact Us\" are removed, ensuring that the final content does not include unnecessary sentences.\n- **Caching of already downloaded pages**: for each webpage, the content is stored in a JSON file, as well as the found sublinks. *Example*: after a run with a limit of N pages, other runs with less than N pages will use the stored files instead downloading data from internet; at the contrary, if the limit is increased to M > N pages, only M - N additional pages will be downloaded while the first N pages will be taken from the stored file.","metadata":{}},{"cell_type":"markdown","source":"## Load the results from the crawler's repo","metadata":{}},{"cell_type":"code","source":"github_downloader.download_files_from_github_repo(folderName=\"\", saveFolder=\"/kaggle/working/companies_info\", extension=\"json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:16.936839Z","iopub.execute_input":"2024-11-23T22:58:16.937170Z","iopub.status.idle":"2024-11-23T22:58:17.917658Z","shell.execute_reply.started":"2024-11-23T22:58:16.937137Z","shell.execute_reply":"2024-11-23T22:58:17.916492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Locate the JSON file containing the companies' information.","metadata":{}},{"cell_type":"code","source":"import os\n\n# if the JSON file is given as Kaggle Input (for example, manually uploaded), change the use_kaggle_input to True\nuse_kaggle_input_companies = False if os.path.exists('/kaggle/working/companies_info') else True\ncompanies_json_name = 'companies_info.json'\nif use_kaggle_input_companies:\n    companies_info_file_path = os.path.join('/kaggle/input/companies-info', companies_json_name)\nelse:\n    companies_info_file_path = os.path.join('/kaggle/working/companies_info', companies_json_name)\n\nprint(f\"The file {companies_info_file_path} will be used for the information regarding the companies\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:17.919396Z","iopub.execute_input":"2024-11-23T22:58:17.919862Z","iopub.status.idle":"2024-11-23T22:58:17.927452Z","shell.execute_reply.started":"2024-11-23T22:58:17.919813Z","shell.execute_reply":"2024-11-23T22:58:17.926215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define a small function to read the information about the companies - in JSON format.","metadata":{}},{"cell_type":"code","source":"import json\n\ndef read_json_info(jsonFilePath: str) -> dict:\n    if os.path.exists(jsonFilePath):\n        with open(jsonFilePath, \"r\") as f:\n            data = json.load(f)\n        return data\n    else:\n        return {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:17.931026Z","iopub.execute_input":"2024-11-23T22:58:17.931384Z","iopub.status.idle":"2024-11-23T22:58:17.944283Z","shell.execute_reply.started":"2024-11-23T22:58:17.931348Z","shell.execute_reply":"2024-11-23T22:58:17.942866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the companies' information by using the defined function.","metadata":{}},{"cell_type":"code","source":"companies_info = read_json_info(companies_info_file_path)\n\n# companies_info is a dictionary, where the key is the name of the company and the related value its information\n# print(companies_info[\"SIEMENS\"]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:17.946171Z","iopub.execute_input":"2024-11-23T22:58:17.946734Z","iopub.status.idle":"2024-11-23T22:58:18.027971Z","shell.execute_reply.started":"2024-11-23T22:58:17.946649Z","shell.execute_reply":"2024-11-23T22:58:18.027012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chat with Gemini","metadata":{}},{"cell_type":"code","source":"# API key got here: https://ai.google.dev/tutorials/setup\n\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n\nuser_secrets = UserSecretsClient()\nsecret_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\ngenai.configure(api_key = secret_key)\n\nmodel_name = 'gemini-1.5-flash-latest'\nmodel = genai.GenerativeModel(model_name=model_name)\n\nchat = model.start_chat()\n\nmodel_info = genai.get_model(f\"models/{model_name}\")\nprint(f\"{model_info.input_token_limit=}\")\nprint(f\"{model_info.output_token_limit=}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:18.029364Z","iopub.execute_input":"2024-11-23T22:58:18.029836Z","iopub.status.idle":"2024-11-23T22:58:19.646853Z","shell.execute_reply.started":"2024-11-23T22:58:18.029787Z","shell.execute_reply":"2024-11-23T22:58:19.644917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyze all the tenders","metadata":{}},{"cell_type":"code","source":"download_tenders_json_from_repo = False # switch to False if willing to generate the json file within this notebook\n\nif download_tenders_json_from_repo:\n    # this helps reducing the Gemini Quota, since no queries will be performed for the tenders\n    github_downloader.download_files_from_github_repo(folderName=\"tenders\", saveFolder=tenders_file_path, extension=\"json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:19.648197Z","iopub.execute_input":"2024-11-23T22:58:19.648856Z","iopub.status.idle":"2024-11-23T22:58:19.654943Z","shell.execute_reply.started":"2024-11-23T22:58:19.648795Z","shell.execute_reply":"2024-11-23T22:58:19.653600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tender_prompt_template_technical = \"\"\"\nYou are an experienced technical tender engineer. \nThe document you have is a tender, that contains also technical requirements for a project.\nThink step by step on how to look for the relevant technical requirements and make a detailed summary.\nThe content of the document is: \"\"\"\ntender_prompts_technical = []\nfor info in tenders_info.values():\n    tender_prompts_technical.append(f\"You have a document called {info['name']} . \" + tender_prompt_template_technical + f\"{info['content']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:19.656087Z","iopub.execute_input":"2024-11-23T22:58:19.656489Z","iopub.status.idle":"2024-11-23T22:58:19.671305Z","shell.execute_reply.started":"2024-11-23T22:58:19.656444Z","shell.execute_reply":"2024-11-23T22:58:19.670102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tender_prompt_template_commercial = \"\"\"\nYou are an experienced commercial tender manager. \nThe document you have is a tender, that contains also commercial requirements for a project.\nThink step by step on how to look for the relevant commercial requirements and make a detailed summary.\nThe content of the document is: \"\n\"\"\"\ntender_prompts_commercial = []\nfor info in tenders_info.values():\n    tender_prompts_commercial.append(f\"You have a document called {info['name']} . \" + tender_prompt_template_commercial + f\"{info['content']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:19.672928Z","iopub.execute_input":"2024-11-23T22:58:19.674152Z","iopub.status.idle":"2024-11-23T22:58:19.686934Z","shell.execute_reply.started":"2024-11-23T22:58:19.673995Z","shell.execute_reply":"2024-11-23T22:58:19.685345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from time import sleep\n\ndef get_responses_tenders(subject, tender_prompts):\n\n    responses = {}\n    tenders_json_file_path = os.path.join(tenders_file_path, f'tenders_{subject}.json')\n    if os.path.exists(tenders_json_file_path):\n        responses = read_json_info(tenders_json_file_path)\n        print(f\"Responses loaded from file {tenders_json_file_path}\")\n    else:\n        num_queries = 0\n        for tender_prompt, tender_name in zip(tender_prompts, tenders):\n            print(f\"Generating response for tender {tender_name} ...\")\n            response = chat.send_message(tender_prompt)\n            # print(response.text)\n            responses[tender_name] = {'prompt': tender_prompt, 'answer': response.text}\n            print(f\"Response for tender {tender_name} generated.\")\n    \n            num_queries += 1\n            \"\"\"\n            if num_queries % 2 == 0:\n                wait_time_seconds = 90\n                print(f\"Waiting {wait_time_seconds} before continuing, to not exceed Gemini's quota\")\n                sleep(wait_time_seconds)\n            \"\"\"\n    \n        with open(tenders_json_file_path, 'w') as f:\n            json.dump(responses, f, ensure_ascii=True, indent=4)\n        print(f\"Responses stored into {tenders_json_file_path}\")\n    \n    print(f\"tender_{subject}: Analysis concluded!\")\n    return responses\n    \nresponse_technical = get_responses_tenders(\"technical\", tender_prompts_technical)\nresponse_commercial = get_responses_tenders(\"commercial\", tender_prompts_commercial)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:58:19.688908Z","iopub.execute_input":"2024-11-23T22:58:19.689425Z","iopub.status.idle":"2024-11-23T22:59:07.441364Z","shell.execute_reply.started":"2024-11-23T22:58:19.689375Z","shell.execute_reply":"2024-11-23T22:59:07.440234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_response_companies_info(company_name):\n    \n    responses = {}\n    company_prompt = f\"these are the information of products and solutions for the company {company_name} : {companies_info[company_name]}\"\n    response = chat.send_message(company_prompt)\n    responses[company_name] = {'prompt': company_prompt, 'answer': response.text}\n\n    print(f\"response for company {company_name} generated!\")\n    return responses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:59:07.442737Z","iopub.execute_input":"2024-11-23T22:59:07.443044Z","iopub.status.idle":"2024-11-23T22:59:07.448982Z","shell.execute_reply.started":"2024-11-23T22:59:07.443014Z","shell.execute_reply":"2024-11-23T22:59:07.447724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response_hitachi = get_response_companies_info(\"HITACHI\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:59:07.450481Z","iopub.execute_input":"2024-11-23T22:59:07.451596Z","iopub.status.idle":"2024-11-23T22:59:26.990155Z","shell.execute_reply.started":"2024-11-23T22:59:07.451549Z","shell.execute_reply":"2024-11-23T22:59:26.988963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\ntime.sleep(60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:59:26.992089Z","iopub.execute_input":"2024-11-23T22:59:26.992559Z","iopub.status.idle":"2024-11-23T23:00:27.057264Z","shell.execute_reply.started":"2024-11-23T22:59:26.992508Z","shell.execute_reply":"2024-11-23T23:00:27.056120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response_siemens = get_response_companies_info(\"SIEMENS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:00:27.058932Z","iopub.execute_input":"2024-11-23T23:00:27.059358Z","iopub.status.idle":"2024-11-23T23:02:32.009039Z","shell.execute_reply.started":"2024-11-23T23:00:27.059312Z","shell.execute_reply":"2024-11-23T23:02:32.007724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# example how to include the chat history here https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_chat.ipynb\n# description of the Content class here https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/GenerativeModel.md\nfrom google.generativeai.protos import Content, Part\n\nhistory_chat = []\n\ndef add_history_to_chat(responses, user, history_chat):\n    for idx_response, response in enumerate(responses.values()):\n        query = Part()\n        query.text = f\"{user}: {response['prompt']}\"\n        # TODO: consider different users? Example: for the tender number idx_response, use f\"user_{idx_response}\"\n        history_chat.append(Content(role=\"user\", parts=[query]))\n    \n        answer = Part()\n        answer.text = response['answer']\n        history_chat.append(Content(role=\"model\", parts=[answer]))\n    return \n\nadd_history_to_chat(response_technical, \"technical engineer\", history_chat)\nadd_history_to_chat(response_commercial, \"commercial manager\", history_chat)\nadd_history_to_chat(response_siemens, \"sales manager for siemens\", history_chat)\nadd_history_to_chat(response_hitachi, \"sales manager for hitachi\", history_chat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:02:32.010996Z","iopub.execute_input":"2024-11-23T23:02:32.011317Z","iopub.status.idle":"2024-11-23T23:02:32.030088Z","shell.execute_reply.started":"2024-11-23T23:02:32.011286Z","shell.execute_reply":"2024-11-23T23:02:32.028879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define the system and user prompts","metadata":{}},{"cell_type":"code","source":"system_prompt = \"\"\"\nYou are an experienced team of business development managers and tender engineers, commercial managers.\nYou need to create a detailed clause by clause from the tender documentations and the most affine company specifications.\n\"\"\"\n\nMarkdown(system_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:02:32.031126Z","iopub.execute_input":"2024-11-23T23:02:32.031397Z","iopub.status.idle":"2024-11-23T23:02:32.047208Z","shell.execute_reply.started":"2024-11-23T23:02:32.031370Z","shell.execute_reply":"2024-11-23T23:02:32.045991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_prompt_match = \"\"\"\n\n1. For company SIEMENS and HITACHI, find the respective relevant products and solutions with respect to the analyzed tenders.\n   The information is in the form of text I provided, then you do not need to read additional documents or access to websites.\n   \n   \n2. Calculate an affinity score in percentage for each company based on analysis in point 1. Explain the way how you computed this percentage.\n\n\"\"\"\n\n# Markdown(user_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:02:32.048645Z","iopub.execute_input":"2024-11-23T23:02:32.049114Z","iopub.status.idle":"2024-11-23T23:02:32.057759Z","shell.execute_reply.started":"2024-11-23T23:02:32.049066Z","shell.execute_reply":"2024-11-23T23:02:32.056567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the history","metadata":{}},{"cell_type":"code","source":"chat_with_memory = model.start_chat(history=history_chat)\nresponse = chat_with_memory.send_message(\"Which are the roles given in the prompt from the user? There are only two for tenders and one for company\")\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:02:32.058910Z","iopub.execute_input":"2024-11-23T23:02:32.059225Z","iopub.status.idle":"2024-11-23T23:03:02.043996Z","shell.execute_reply.started":"2024-11-23T23:02:32.059194Z","shell.execute_reply":"2024-11-23T23:03:02.042977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#response = chat_with_memory.send_message(\"Detailed explanation of the technical specifications of the tenders\")\n\n#Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:03:02.045447Z","iopub.execute_input":"2024-11-23T23:03:02.045922Z","iopub.status.idle":"2024-11-23T23:03:02.051004Z","shell.execute_reply.started":"2024-11-23T23:03:02.045873Z","shell.execute_reply":"2024-11-23T23:03:02.049795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Count the needed tokens","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate the final response","metadata":{}},{"cell_type":"code","source":"print(\"Finding the most suitable company for the tenders ...\")\nmatch_prompt = f\"{user_prompt_match}\"\nmatch_response = chat_with_memory.send_message(match_prompt)\n# print(response.text)\n\nprint(\"Response to the prompts is ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:03:02.052830Z","iopub.execute_input":"2024-11-23T23:03:02.053130Z","iopub.status.idle":"2024-11-23T23:04:26.916142Z","shell.execute_reply.started":"2024-11-23T23:03:02.053101Z","shell.execute_reply":"2024-11-23T23:04:26.914852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Markdown(match_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:04:26.917905Z","iopub.execute_input":"2024-11-23T23:04:26.918355Z","iopub.status.idle":"2024-11-23T23:04:26.926244Z","shell.execute_reply.started":"2024-11-23T23:04:26.918306Z","shell.execute_reply":"2024-11-23T23:04:26.924970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_prompt = \"\"\"\n\nConsider the company with the highest affinity score \nand return the clause by clause analysis considering technical and commercial compliant and not-compliant requirements\nof the tender with respect to the selected company. Report also the URL of the source where you found the informations.\n\n\"\"\"\n\n# Markdown(user_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:04:26.930488Z","iopub.execute_input":"2024-11-23T23:04:26.930891Z","iopub.status.idle":"2024-11-23T23:04:26.943121Z","shell.execute_reply.started":"2024-11-23T23:04:26.930855Z","shell.execute_reply":"2024-11-23T23:04:26.942016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"{model.count_tokens(history_chat)=}\")\nprint(f\"{model.count_tokens(system_prompt)=}\")\nprint(f\"{model.count_tokens(user_prompt)=}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:04:26.944642Z","iopub.execute_input":"2024-11-23T23:04:26.945050Z","iopub.status.idle":"2024-11-23T23:04:37.891423Z","shell.execute_reply.started":"2024-11-23T23:04:26.945016Z","shell.execute_reply":"2024-11-23T23:04:37.890288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Generating the clause by clause ...\")\nfinal_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\nfinal_response = chat_with_memory.send_message(final_prompt)\n# print(response.text)\n\nprint(\"Response to the prompts is ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:04:37.892729Z","iopub.execute_input":"2024-11-23T23:04:37.893626Z","iopub.status.idle":"2024-11-23T23:06:01.392321Z","shell.execute_reply.started":"2024-11-23T23:04:37.893590Z","shell.execute_reply":"2024-11-23T23:06:01.390923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Markdown(final_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T23:06:01.393733Z","iopub.execute_input":"2024-11-23T23:06:01.394083Z","iopub.status.idle":"2024-11-23T23:06:01.401399Z","shell.execute_reply.started":"2024-11-23T23:06:01.394049Z","shell.execute_reply":"2024-11-23T23:06:01.400281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}