{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba98b2b4",
   "metadata": {
    "papermill": {
     "duration": 0.004501,
     "end_time": "2024-11-12T21:45:59.638397",
     "exception": false,
     "start_time": "2024-11-12T21:45:59.633896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define functions to scrape websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e8a94",
   "metadata": {
    "papermill": {
     "duration": 0.003743,
     "end_time": "2024-11-12T21:45:59.646385",
     "exception": false,
     "start_time": "2024-11-12T21:45:59.642642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install the required dependencies to scrape websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093168d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:45:59.656245Z",
     "iopub.status.busy": "2024-11-12T21:45:59.655612Z",
     "iopub.status.idle": "2024-11-12T21:46:23.257960Z",
     "shell.execute_reply": "2024-11-12T21:46:23.256961Z"
    },
    "papermill": {
     "duration": 23.609763,
     "end_time": "2024-11-12T21:46:23.260237",
     "exception": false,
     "start_time": "2024-11-12T21:45:59.650474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.8.30)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3051ba21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:23.270867Z",
     "iopub.status.busy": "2024-11-12T21:46:23.270544Z",
     "iopub.status.idle": "2024-11-12T21:46:23.555766Z",
     "shell.execute_reply": "2024-11-12T21:46:23.555040Z"
    },
    "papermill": {
     "duration": 0.293208,
     "end_time": "2024-11-12T21:46:23.558025",
     "exception": false,
     "start_time": "2024-11-12T21:46:23.264817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from collections import deque\n",
    "\n",
    "VALID_GET_RESPONSE = 200\n",
    "\n",
    "\n",
    "def soup_page(url: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Function to make a HTTP request by a given url\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == VALID_GET_RESPONSE:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    else:\n",
    "        soup = BeautifulSoup(\"\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_links(soup: BeautifulSoup, url: str, parentFolder: str) -> list:\n",
    "    \"\"\"\n",
    "    Function to get all links from a page\n",
    "    \"\"\"\n",
    "    links = [\n",
    "        a[\"href\"] for a in soup.find_all(\"a\", href=True) if parentFolder in a[\"href\"] and not \"contact-us\" in a[\"href\"]\n",
    "    ]\n",
    "    return links\n",
    "\n",
    "\n",
    "def download_page(soup: BeautifulSoup, url: str, savePath: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to download a page\n",
    "    \"\"\"\n",
    "    paragraphsList = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "    pageTitle = [t.get_text() for t in soup.find_all(\"title\")]\n",
    "    if paragraphsList:\n",
    "        siteData = {\"title\": pageTitle, \"url\": url, \"content\": paragraphsList}\n",
    "\n",
    "        urlPath = url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")\n",
    "        filePath = os.path.join(savePath, urlPath + \".json\")\n",
    "        write_json_from_data(siteData, filePath)\n",
    "        return filePath\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def write_json_from_data(data: dict, filePath: str, indentSize: int = 4) -> None:\n",
    "    \"\"\"\n",
    "    Function to write a json file with its data\n",
    "    \"\"\"\n",
    "    with open(filePath, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=indentSize)\n",
    "    return\n",
    "\n",
    "\n",
    "def download_all_pages(\n",
    "    rootUrl: str, parentFolder: str, savePath: str, maxPages: int = 100\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Recursive function to download pages and subpages\n",
    "    \"\"\"\n",
    "    visitedUrls = set()\n",
    "    stack = deque([rootUrl])\n",
    "\n",
    "    jsonFiles = []\n",
    "    while stack and len(jsonFiles) < maxPages:\n",
    "        url = stack.pop()\n",
    "\n",
    "        if url in visitedUrls:\n",
    "            continue\n",
    "        visitedUrls.add(url)\n",
    "\n",
    "        soup = soup_page(url)\n",
    "        jsonFileFromSite = download_page(soup, url, savePath)\n",
    "        print(f\"JSON file {jsonFileFromSite} created from url {url}\")\n",
    "        jsonFiles.append(jsonFileFromSite)\n",
    "\n",
    "        links = get_links(soup, url, parentFolder)\n",
    "        for link in links:\n",
    "            fullLink = urljoin(url, link)\n",
    "            if fullLink not in visitedUrls and \"http\" in fullLink:\n",
    "                stack.append(fullLink)\n",
    "\n",
    "    return jsonFiles\n",
    "\n",
    "\n",
    "def most_common_sentences_in_file(\n",
    "    jsonFilePath: str, alreadyCommonWords: set = set(), frequencyThreshold: int = 1\n",
    ") -> list[str]:\n",
    "    with open(jsonFilePath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if type(data) == list:\n",
    "        contentList = [c.get(\"content\", \"\") for c in data]\n",
    "        content = []\n",
    "        for contentPage in contentList:\n",
    "            content.extend(contentPage)\n",
    "    else:\n",
    "        content = data.get(\"content\", \"\")\n",
    "\n",
    "    frequencies = {}\n",
    "    if content:\n",
    "        for sentence in content:\n",
    "            frequencies[sentence] = frequencies.get(sentence, 0) + 1\n",
    "\n",
    "    for sentence, freq in frequencies.items():\n",
    "        if freq > frequencyThreshold:\n",
    "            alreadyCommonWords.add(sentence)\n",
    "    return alreadyCommonWords\n",
    "\n",
    "\n",
    "def list_json_files_in_folder(\n",
    "    jsonFilesFolder: str, jsonFilesToExclude: str | list[str]\n",
    ") -> list[str]:\n",
    "    if type(jsonFilesToExclude) == str:\n",
    "        jsonFilesToExclude = [jsonFilesToExclude]\n",
    "\n",
    "    filesToExclude = set(jsonFilesToExclude)\n",
    "    fileList = [\n",
    "        os.path.join(jsonFilesFolder, file)\n",
    "        for file in os.listdir(jsonFilesFolder)\n",
    "        if file.endswith(\".json\") and file not in filesToExclude\n",
    "    ]\n",
    "    return fileList\n",
    "\n",
    "\n",
    "def clean_json_file(\n",
    "    jsonFilePath: str, mostCommonRows: set[str] = set(), overwrite: bool = True\n",
    ") -> str:\n",
    "    with open(jsonFilePath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if type(data) == list:\n",
    "        for i, page in enumerate(data):\n",
    "            indexesToRemove = set()\n",
    "            for j, row in enumerate(page.get(\"content\", \"\")):\n",
    "                if row in mostCommonRows:\n",
    "                    indexesToRemove.add(j)\n",
    "\n",
    "            data[i][\"content\"][:] = [\n",
    "                c for k, c in enumerate(data[i][\"content\"]) if not k in indexesToRemove\n",
    "            ]\n",
    "    else:\n",
    "        indexesToRemove = set()\n",
    "        for i, row in enumerate(data.get(\"content\", \"\")):\n",
    "            if row in mostCommonRows:\n",
    "                indexesToRemove.add(i)\n",
    "\n",
    "        data[\"content\"][:] = [\n",
    "            c for k, c in enumerate(data[\"content\"]) if k not in indexesToRemove\n",
    "        ]\n",
    "\n",
    "    if overwrite:\n",
    "        jsonCleanedFilePath = jsonFilePath\n",
    "    else:\n",
    "        jsonCleanedFilePath = jsonFilePath.replace(\".json\", \"_cleaned.json\")\n",
    "\n",
    "    write_json_from_data(data, jsonCleanedFilePath)\n",
    "    return jsonCleanedFilePath\n",
    "\n",
    "\n",
    "def clean_json_files(\n",
    "    jsonFilesFolder: str, filesToExclude: str | list[str], overwrite: bool = True\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Function to remove unneded text from a json file containing web page content\n",
    "    \"\"\"\n",
    "    fileList = list_json_files_in_folder(jsonFilesFolder, filesToExclude)\n",
    "\n",
    "    mostCommonRows = set()  # it is possible to define custom common words here\n",
    "    for file in fileList:\n",
    "        mostCommonRows = most_common_sentences_in_file(file, mostCommonRows)\n",
    "\n",
    "    cleanedFiles = []\n",
    "    for file in fileList:\n",
    "        jsonCleanedFilePath = clean_json_file(file, mostCommonRows, overwrite)\n",
    "        cleanedFiles.append(jsonCleanedFilePath)\n",
    "    return cleanedFiles\n",
    "\n",
    "\n",
    "def merge_json_files(jsonFilesFolder: str, targetFile: str = \"_merged.json\") -> str:\n",
    "    \"\"\"\n",
    "    Function to merge multiple json files into a single one\n",
    "    \"\"\"\n",
    "    fileList = list_json_files_in_folder(jsonFilesFolder, targetFile)\n",
    "    if fileList:\n",
    "        resultFileContent = []\n",
    "        for file in fileList:\n",
    "            with open(file, \"r\") as f:\n",
    "                jsonData = json.load(f)\n",
    "                resultFileContent.append(jsonData)\n",
    "\n",
    "        resultFilePath = os.path.join(jsonFilesFolder, targetFile)\n",
    "        write_json_from_data(resultFileContent, resultFilePath)\n",
    "        return resultFilePath\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def json_to_txt(savePath: str, jsonFilePath: str, targetName: str = \"\") -> str:\n",
    "    if targetName == \"\":\n",
    "        targetName = os.path.basename(jsonFilePath).replace(\".json\", \".txt\")\n",
    "\n",
    "    with open(jsonFilePath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    resultFilePath = os.path.join(savePath, targetName)\n",
    "    with open(resultFilePath, \"w\") as f:\n",
    "        for numPage, webpage in enumerate(data):\n",
    "            pageTitle = webpage.get(\"title\", \"\")\n",
    "            f.write(\n",
    "                f'The page number {numPage+1} with name \"{pageTitle[0]}\" has the following content:\\n'\n",
    "            )\n",
    "\n",
    "            pageContent = webpage.get(\"content\", \"\")\n",
    "            for row in pageContent:\n",
    "                f.write(f\"{row} \\n\")\n",
    "\n",
    "            f.write(\n",
    "                f'The page number {numPage+1} with name \"{pageTitle[0]}\" ends here.\\n\\n'\n",
    "            )\n",
    "\n",
    "    return resultFilePath\n",
    "\n",
    "\n",
    "def text_from_file(filePath: str) -> str | list[str]:\n",
    "    with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
    "        textString = f.read()\n",
    "        f.seek(0)\n",
    "        textList = f.readlines()\n",
    "    return textString, textList\n",
    "\n",
    "\n",
    "def get_text_from_webpages(\n",
    "    root: str, parentFolder: str, savePath: str = \"crawled\", numPages: int = 5\n",
    ") -> str:\n",
    "    os.makedirs(savePath, exist_ok=True)\n",
    "\n",
    "    jsonFiles = download_all_pages(root, parentFolder, savePath, numPages)\n",
    "\n",
    "    mergedJsonName = \"_merged.json\"\n",
    "    jsonFilesClean = clean_json_files(savePath, mergedJsonName)\n",
    "    mergedJsonPath = merge_json_files(savePath, mergedJsonName)\n",
    "\n",
    "    mostCommonWordsMergedJson = most_common_sentences_in_file(\n",
    "        mergedJsonPath, frequencyThreshold=len(jsonFilesClean) - 1\n",
    "    )\n",
    "    mergedJsonPath = clean_json_file(mergedJsonPath, mostCommonWordsMergedJson)\n",
    "\n",
    "    plainTextFile = json_to_txt(savePath, mergedJsonPath)\n",
    "    plainText, textString = text_from_file(plainTextFile)\n",
    "    return plainText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f01669",
   "metadata": {
    "papermill": {
     "duration": 0.004126,
     "end_time": "2024-11-12T21:46:23.566570",
     "exception": false,
     "start_time": "2024-11-12T21:46:23.562444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Call the function to scrape the website\n",
    "\n",
    "Function arguments of **get_text_from_webpages()** are defined as:\n",
    "* root : initial webpage, to be scraped and whose links will be scraped, as well\n",
    "* parentFolder : website folder to inspect, external webpages in the site structure will be skipped\n",
    "* savePath : folder where to save .json files, containing the content of the webpages\n",
    "* numPages : maximum number of webpages to download, starting from the initial page (root argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbb45bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:23.577427Z",
     "iopub.status.busy": "2024-11-12T21:46:23.576999Z",
     "iopub.status.idle": "2024-11-12T21:46:53.509310Z",
     "shell.execute_reply": "2024-11-12T21:46:53.508423Z"
    },
    "papermill": {
     "duration": 29.940712,
     "end_time": "2024-11-12T21:46:53.511498",
     "exception": false,
     "start_time": "2024-11-12T21:46:23.570786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_.json created from url https://www.hitachienergy.com/products-and-solutions/\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_cybersecurity_alerts-and-notifications.json created from url https://www.hitachienergy.com/products-and-solutions/cybersecurity/alerts-and-notifications\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_cybersecurity.json created from url https://www.hitachienergy.com/products-and-solutions/cybersecurity\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_cybersecurity_reporting.json created from url https://www.hitachienergy.com/products-and-solutions/cybersecurity/reporting\n",
      "JSON file  created from url https://www.hitachienergy.com/products-and-solutions/cybersecurity/reporting/report-a-vulnerability-web-form\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_digitalization.json created from url https://www.hitachienergy.com/products-and-solutions/digitalization\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_grid-edge-solutions.json created from url https://www.hitachienergy.com/products-and-solutions/grid-edge-solutions\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_energy-portfolio-management.json created from url https://www.hitachienergy.com/products-and-solutions/energy-portfolio-management\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_energy-portfolio-management_enterprise_capacity-expansion.json created from url https://www.hitachienergy.com/products-and-solutions/energy-portfolio-management/enterprise/capacity-expansion\n",
      "JSON file crawled/www.hitachienergy.com_products-and-solutions_energy-portfolio-management_enterprise.json created from url https://www.hitachienergy.com/products-and-solutions/energy-portfolio-management/enterprise\n"
     ]
    }
   ],
   "source": [
    "parent_folder = \"/products-and-solutions/\"\n",
    "initial_url = \"https://www.hitachienergy.com/products-and-solutions/\"\n",
    "save_folder = \"crawled\"\n",
    "num_pages_to_download = 10\n",
    "parsed_text = get_text_from_webpages(\n",
    "    root=initial_url, parentFolder=parent_folder, savePath=save_folder,numPages=num_pages_to_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0c3e",
   "metadata": {
    "papermill": {
     "duration": 0.004927,
     "end_time": "2024-11-12T21:46:53.521738",
     "exception": false,
     "start_time": "2024-11-12T21:46:53.516811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The variable **parsedText** contains the merged content from all the pages.\n",
    "\n",
    "To distinguish between pages and help Gemini recognizing them, two sentences are added:\n",
    "* Before the content of each webpage\n",
    "* After the content of each webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262afabe",
   "metadata": {
    "papermill": {
     "duration": 0.004872,
     "end_time": "2024-11-12T21:46:53.531633",
     "exception": false,
     "start_time": "2024-11-12T21:46:53.526761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start a chat with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd752376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:53.542951Z",
     "iopub.status.busy": "2024-11-12T21:46:53.542649Z",
     "iopub.status.idle": "2024-11-12T21:46:54.674968Z",
     "shell.execute_reply": "2024-11-12T21:46:54.674214Z"
    },
    "papermill": {
     "duration": 1.140527,
     "end_time": "2024-11-12T21:46:54.677153",
     "exception": false,
     "start_time": "2024-11-12T21:46:53.536626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API key got here: https://ai.google.dev/tutorials/setup\n",
    "\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key = api_key)\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-pro-latest')\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "# example how to send prompts\n",
    "# response = chat.send_message('Recommend me ways to fetch information from a website and forward it to you')\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35952c10",
   "metadata": {
    "papermill": {
     "duration": 0.00526,
     "end_time": "2024-11-12T21:46:54.687887",
     "exception": false,
     "start_time": "2024-11-12T21:46:54.682627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pass the content of the website to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8486f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:54.699377Z",
     "iopub.status.busy": "2024-11-12T21:46:54.698959Z",
     "iopub.status.idle": "2024-11-12T21:46:54.703752Z",
     "shell.execute_reply": "2024-11-12T21:46:54.702895Z"
    },
    "papermill": {
     "duration": 0.013073,
     "end_time": "2024-11-12T21:46:54.706064",
     "exception": false,
     "start_time": "2024-11-12T21:46:54.692991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system prompt is:\n",
      "You are given the content of a website.\n",
      "The content for each webpage of the website is after the sentence \"The page number X with name PAGENAME has the following content: \" and before the sentence \"The page number X with name PAGENAME ends here.\" where X is the number of the page and PAGENAME is the name of the webpage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are given the content of a website.\\nThe content for each webpage of the website is after the sentence \\\"The page number X with name PAGENAME has the following content: \\\" and before the sentence \\\"The page number X with name PAGENAME ends here.\\\" where X is the number of the page and PAGENAME is the name of the webpage.\\n\"\n",
    "print(f\"The system prompt is:\\n{system_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a74a611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:54.718157Z",
     "iopub.status.busy": "2024-11-12T21:46:54.717374Z",
     "iopub.status.idle": "2024-11-12T21:46:54.721177Z",
     "shell.execute_reply": "2024-11-12T21:46:54.720492Z"
    },
    "papermill": {
     "duration": 0.011683,
     "end_time": "2024-11-12T21:46:54.722997",
     "exception": false,
     "start_time": "2024-11-12T21:46:54.711314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_prompt = \"Use the content of the website to illustrate the products of the company.\\nHighlight the possible business areas in which the product can be sold competitively.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e35e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:54.734453Z",
     "iopub.status.busy": "2024-11-12T21:46:54.734195Z",
     "iopub.status.idle": "2024-11-12T21:46:54.737820Z",
     "shell.execute_reply": "2024-11-12T21:46:54.736960Z"
    },
    "papermill": {
     "duration": 0.011476,
     "end_time": "2024-11-12T21:46:54.739746",
     "exception": false,
     "start_time": "2024-11-12T21:46:54.728270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "information = f\"The content of the website is:\\n{parsed_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88f7c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:46:54.751472Z",
     "iopub.status.busy": "2024-11-12T21:46:54.751202Z",
     "iopub.status.idle": "2024-11-12T21:47:10.159804Z",
     "shell.execute_reply": "2024-11-12T21:47:10.158845Z"
    },
    "papermill": {
     "duration": 15.416936,
     "end_time": "2024-11-12T21:47:10.162000",
     "exception": false,
     "start_time": "2024-11-12T21:46:54.745064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitachi Energy offers a suite of products and solutions centered around digitalization and optimization of the energy sector. Here's a breakdown based on the provided website content:\n",
      "\n",
      "**Products and Solutions:**\n",
      "\n",
      "* **Cybersecurity Solutions:** Automated, evolving, and resilient cybersecurity solutions designed to meet international standards and protect against emerging threats. This includes incident reporting and vulnerability management.\n",
      "* **Digitalization Solutions:**  Focuses on managing the increasingly complex energy landscape through digital tools and data analysis.  This applies to asset management, renewable energy integration, and optimization of operations for various industries like mining, metals, oil and gas, and transportation.  The e-mesh™ portfolio plays a key role here.\n",
      "* **Capacity Expansion Solution:** Software for resource planning, capacity expansion, and emissions compliance planning.  Helps manage long-term resource plans, analyze renewable portfolio standards, and evaluate resource acquisitions.\n",
      "* **Market Analytics Tools:** Provide rigorously curated data and sophisticated analysis tools for forecasting electricity and fuel prices, performing planning activities, valuing assets, and managing organizational risk.\n",
      "* **Substation Automation, Protection & Control:**  Specific products for automating and controlling substations.\n",
      "* **Grid Edge Solutions (e-mesh™ portfolio):**  Focuses on energy management and optimization through battery storage systems, automated and digital capabilities and services. Key applications include microgrids, renewable energy integration, and EV charging infrastructure management.\n",
      "* **Energy Portfolio Management:**  Software and services for planning, forecasting, trading, portfolio optimization, and market operations.  Caters to utilities, developers, transmission planners, market operators, traders, private enterprises, and regulatory agencies.  Covers areas like renewable energy investment, REC trading, wind and solar forecasting, and production cost modeling.\n",
      "\n",
      "**Potential Business Areas and Competitive Advantages:**\n",
      "\n",
      "* **Utilities:**  Hitachi Energy's digitalization solutions, grid edge offerings, and cybersecurity expertise are highly relevant to utilities facing the challenges of the energy transition. Competitive advantages include the ability to integrate renewable energy sources, optimize grid operations, and enhance cybersecurity posture.\n",
      "* **Renewable Energy Developers:**  Capacity expansion planning, market analytics, and grid edge solutions are valuable for developers of renewable energy projects.  The ability to accurately forecast energy prices, optimize site selection, and integrate projects into the grid provides a competitive edge.\n",
      "* **Oil & Gas, Mining, and Metals Industries:**  Hitachi Energy's digitalization solutions can help these asset-intensive industries optimize operations, improve asset lifespan, and reduce environmental impact. The focus on data-driven decision-making and automation can be a significant differentiator.\n",
      "* **Transportation Sector (Air, eMobility, Marine, Rail):** Digitalization solutions can empower transportation operators to improve visibility over their networks, respond to outages, and manage energy consumption.  The e-mesh™ portfolio, with its focus on EV charging and grid stability, is particularly relevant.\n",
      "* **Energy Traders and Market Operators:**  The market analytics tools and energy portfolio management solutions are crucial for effective trading and risk management in the increasingly complex energy market.  Hitachi Energy's deep expertise and comprehensive data sets can provide a competitive advantage.\n",
      "* **Government and Regulatory Agencies:**  Capacity expansion planning tools and market analytics can assist in developing effective energy policies and regulations.  Hitachi Energy's independent and unbiased data can be valuable for informed decision-making.\n",
      "\n",
      "\n",
      "By focusing on digitalization, cybersecurity, and optimization across the energy value chain, Hitachi Energy is well-positioned to compete in a rapidly evolving market.  Their comprehensive suite of products and solutions caters to a broad range of customers and addresses key challenges faced by the industry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(system_prompt + user_prompt + information)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc7b35d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:47:10.175217Z",
     "iopub.status.busy": "2024-11-12T21:47:10.174435Z",
     "iopub.status.idle": "2024-11-12T21:47:10.179470Z",
     "shell.execute_reply": "2024-11-12T21:47:10.178600Z"
    },
    "papermill": {
     "duration": 0.01345,
     "end_time": "2024-11-12T21:47:10.181374",
     "exception": false,
     "start_time": "2024-11-12T21:47:10.167924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 3741\n"
     ]
    }
   ],
   "source": [
    "usage_metadata = response.usage_metadata\n",
    "print(f\"Token count: {usage_metadata.total_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0cb76a",
   "metadata": {
    "papermill": {
     "duration": 0.00556,
     "end_time": "2024-11-12T21:47:10.192702",
     "exception": false,
     "start_time": "2024-11-12T21:47:10.187142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 73.582181,
   "end_time": "2024-11-12T21:47:10.517713",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-12T21:45:56.935532",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
